{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iw8FouuJegh6",
    "outputId": "fc7b7503-2e7d-4621-dfa4-6f1fcd3cca8b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets pytorch_lightning rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9kdIyyIwKow",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ev-eO_NE5L2f",
    "outputId": "144003ec-5989-47d7-8c13-0a82fa274db1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChartQADataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "JEcB_a9bdPRn",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@title ChartQADataset\n",
    "\n",
    "import json, os\n",
    "import random\n",
    "from typing import Any, List, Tuple\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DonutProcessor\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import io\n",
    "\n",
    "added_tokens = []\n",
    "\n",
    "class ChartQADataset(Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: str,\n",
    "        images_folder: str,\n",
    "        max_length: int,\n",
    "        processor : DonutProcessor = None,\n",
    "        split: str = \"train\",\n",
    "        ignore_id: int = -100,\n",
    "        prompt_end_token: str = None,\n",
    "        task_prefix: str = '<chartqa>',\n",
    "        sort_json_key: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.ignore_id = ignore_id\n",
    "\n",
    "        self.prompt_end_token = prompt_end_token\n",
    "        self.sort_json_key = sort_json_key\n",
    "        self.images_folder = images_folder\n",
    "\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.dataset_length = len(self.dataset)\n",
    "\n",
    "        self.processor = processor\n",
    "        self.prompt_end_token_id = self.processor.tokenizer.convert_tokens_to_ids(self.prompt_end_token)\n",
    "        self.task_prefix = task_prefix\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        # input_tensor\n",
    "        image = sample['image']\n",
    "        if isinstance(image, str):\n",
    "            image = eval(image)\n",
    "        # img = Image.open(io.BytesIO(image))\n",
    "        img = sample['image']\n",
    "        pixel_values = self.processor(img.convert(\"RGB\"), random_padding=self.split == \"train\", return_tensors=\"pt\").pixel_values\n",
    "        input_tensor = pixel_values.squeeze()\n",
    "\n",
    "        # input_ids\n",
    "        processed_parse = (self.task_prefix + \" \" + str(sample['query']) + \" \" +\n",
    "                           '<s_rationale>' + \" \" + str(sample['rationale']) + \" \" +\n",
    "                          \"<s_answer>\" + \" \" + str(sample['label']) + self.processor.tokenizer.eos_token)\n",
    "\n",
    "        input_ids = self.processor.tokenizer(\n",
    "            processed_parse,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        answer_token_id = self.processor.tokenizer.convert_tokens_to_ids(\"<s_answer>\")\n",
    "        rationale_token_id = self.processor.tokenizer.convert_tokens_to_ids(\"<s_rationale>\")\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            input_ids = {}\n",
    "            input_ids['ans_input_ids'] = self.processor.tokenizer(\n",
    "                (self.task_prefix + \" \" + str(sample['query']) + \" \" + \"<s_answer>\" + \" \" + str(sample['label']).capitalize() + self.processor.tokenizer.eos_token),\n",
    "                add_special_tokens=False,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "            answer_labels = input_ids['ans_input_ids'].clone()\n",
    "            answer_labels[\n",
    "                answer_labels == self.processor.tokenizer.pad_token_id\n",
    "            ] = self.ignore_id  # model doesn't need to predict pad token\n",
    "            answer_labels[\n",
    "                : torch.nonzero(answer_labels == answer_token_id).sum()\n",
    "            ] = self.ignore_id  # model doesn't need to predict prompt\n",
    "\n",
    "            input_ids['rat_input_ids'] = self.processor.tokenizer(\n",
    "                (self.task_prefix + \" \" + str(sample['query']) + \" \" + \"<s_rationale>\" + \" \" + str(sample['rationale']) + self.processor.tokenizer.eos_token),\n",
    "                add_special_tokens=False,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "\n",
    "            rationale_labels = input_ids['rat_input_ids'].clone()\n",
    "            rationale_labels[\n",
    "                rationale_labels == self.processor.tokenizer.pad_token_id\n",
    "            ] = self.ignore_id  # model doesn't need to predict pad token\n",
    "            rationale_labels[\n",
    "                : torch.nonzero(rationale_labels == rationale_token_id).sum()\n",
    "            ] = self.ignore_id  # model doesn't need to predict prompt\n",
    "\n",
    "\n",
    "            return input_tensor, input_ids, rationale_labels, answer_labels\n",
    "        else:\n",
    "            rationale_idx = torch.nonzero(\n",
    "                input_ids == rationale_token_id\n",
    "            ).sum()\n",
    "\n",
    "            answer_idx = torch.nonzero(\n",
    "                input_ids == answer_token_id\n",
    "            ).sum()\n",
    "\n",
    "            prompt_end_index = (rationale_idx, answer_idx)\n",
    "            return input_tensor, input_ids, prompt_end_index, processed_parse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChartQAModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "AFAbgLdq8kuy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@title ChartQAModule\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "from nltk import edit_distance\n",
    "import numpy as np\n",
    "import math, os\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "\n",
    "\n",
    "class ChartQAModule(pl.LightningModule):\n",
    "    def __init__(self, config, processor, model, args, train_dataset, val_dataset):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.args=args\n",
    "        self.validation_step_outputs = []\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "        self.rationale_rouge_outputs = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pixel_values, decoder_input_ids, rationale_labels, answer_labels = batch\n",
    "\n",
    "        # print(processor.tokenizer.batch_decode(decoder_input_ids['ans_input_ids'][:, :-1]))\n",
    "        # print(processor.tokenizer.batch_decode([i for i in answer_labels[:, 1:][0] if i != -100]))\n",
    "        # print(processor.tokenizer.batch_decode(decoder_input_ids['rat_input_ids'][:, :-1]))\n",
    "        # print(processor.tokenizer.batch_decode([i for i in rationale_labels[:, 1:][0] if i != -100]))\n",
    "        \n",
    "        answers = self.model(pixel_values,\n",
    "                             decoder_input_ids=decoder_input_ids['ans_input_ids'][:, :],\n",
    "                             labels=answer_labels[:, :])\n",
    "        rationales = self.model(pixel_values,\n",
    "                                decoder_input_ids=decoder_input_ids['rat_input_ids'][:, :],\n",
    "                                labels=rationale_labels[:, :])\n",
    "        alpha = 0.5\n",
    "        loss = (1 - alpha) * answers.loss + alpha * rationales.loss\n",
    "        print(\"Train loss: \", loss)\n",
    "        self.log_dict({\"train_loss\": loss}, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def compute_metric(self, gt, pred):\n",
    "      try:\n",
    "        gt = float(gt)\n",
    "        pred = float(pred)\n",
    "        return abs(gt - pred) / abs(gt) <= 0.05\n",
    "      except:\n",
    "        return str(gt).lower() == str(pred).lower()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "        pixel_values, decoder_input_ids, prompt_end_idxs, answers = batch\n",
    "\n",
    "        ans_decoder_prompts = pad_sequence(\n",
    "            [torch.cat([input_id[: prompt_end_idxs[0][idx].item()], input_id[prompt_end_idxs[1][idx].item(): prompt_end_idxs[1][idx].item() + 1]],dim=0) for idx, input_id in enumerate(decoder_input_ids)],\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        rat_decoder_prompts = pad_sequence(\n",
    "            [input_id[: prompt_end_idxs[0][idx].item() + 1] for idx, input_id in enumerate(decoder_input_ids)],\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        ans_outputs = self.model.generate(pixel_values.to(device),\n",
    "                                   decoder_input_ids=ans_decoder_prompts.to(device),\n",
    "                                   max_length=self.args.max_length,\n",
    "                                   early_stopping=True,\n",
    "                                   pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                                   eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "                                   use_cache=True,\n",
    "                                   num_beams=4,\n",
    "                                   bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n",
    "                                   return_dict_in_generate=True,)\n",
    "\n",
    "        rat_outputs = self.model.generate(pixel_values.to(device),\n",
    "                           decoder_input_ids=rat_decoder_prompts.to(device),\n",
    "                           max_length=self.args.max_length,\n",
    "                           early_stopping=True,\n",
    "                           pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                           eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "                           use_cache=True,\n",
    "                           num_beams=4,\n",
    "                           bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n",
    "                           return_dict_in_generate=True,)\n",
    "\n",
    "        predictions = []\n",
    "        for seq in self.processor.tokenizer.batch_decode(ans_outputs.sequences):\n",
    "            seq = seq.replace(self.processor.tokenizer.eos_token, \"\").replace(self.processor.tokenizer.pad_token, \"\")\n",
    "            predictions.append(seq)\n",
    "\n",
    "        rationale_predictions = []\n",
    "        for seq in self.processor.tokenizer.batch_decode(rat_outputs.sequences):\n",
    "            seq = seq.replace(self.processor.tokenizer.eos_token, \"\").replace(self.processor.tokenizer.pad_token, \"\")\n",
    "            rationale_predictions.append(seq)\n",
    "            \n",
    "        # print(predictions)\n",
    "        scores = list()\n",
    "        for pred, rat_pred, answer in zip(predictions, rationale_predictions, answers):\n",
    "            # Extract answer\n",
    "            pred_ans = pred.split(\"<s_answer>\")[1] if \"<s_answer>\" in pred else \"\"\n",
    "            pred_ans = pred_ans.replace(self.processor.tokenizer.eos_token, \"\").replace(\"<s>\", \"\").strip()\n",
    "        \n",
    "            gold_ans = answer.split(\"<s_answer>\")[1] if \"<s_answer>\" in answer else \"\"\n",
    "            gold_ans = gold_ans.replace(self.processor.tokenizer.eos_token, \"\").strip()\n",
    "        \n",
    "            # Extract rationale\n",
    "            gold_rat = answer.split(\"<s_answer>\")[0] if \"<s_answer>\" in answer else \"\"\n",
    "            gold_rat = gold_rat.split(\"<s_rationale>\")[1] if \"<s_rationale>\" in gold_rat else \"\"\n",
    "            gold_rat = gold_rat.replace(self.processor.tokenizer.eos_token, \"\").replace(\"<s>\", \"\").strip()\n",
    "\n",
    "            rat_pred = rat_pred.split(\"<s_rationale>\")[1] if \"<s_rationale>\" in rat_pred else \"\"\n",
    "            rat_pred = rat_pred.replace(self.processor.tokenizer.eos_token, \"\").replace(\"<s>\", \"\").strip()\n",
    "        \n",
    "            # Accuracy for answers\n",
    "            print(\"Gold ans: \", gold_ans)\n",
    "            print(\"Pred ans: \", pred_ans)\n",
    "            \n",
    "            if self.compute_metric(gold_ans, pred_ans):\n",
    "                scores.append(1)\n",
    "            else:\n",
    "                scores.append(0)\n",
    "\n",
    "            print(\"Gold rat: \", gold_rat)\n",
    "            print(\"Pred rat: \", rat_pred)\n",
    "            # ROUGE for rationale\n",
    "            rouge_scores = self.rouge_scorer.score(gold_rat, rat_pred)\n",
    "            self.rationale_rouge_outputs.append(rouge_scores)\n",
    "    \n",
    "        self.validation_step_outputs.append(scores)\n",
    "        return scores\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        validation_step_outputs = self.validation_step_outputs\n",
    "        # I set this to 1 manually\n",
    "        # (previously set to len(self.config.dataset_name_or_paths))\n",
    "        num_of_loaders = 1\n",
    "        if num_of_loaders == 1:\n",
    "            validation_step_outputs = [validation_step_outputs]\n",
    "        assert len(validation_step_outputs) == num_of_loaders\n",
    "        cnt = [0] * num_of_loaders\n",
    "        total_metric = [0] * num_of_loaders\n",
    "        val_metric = [0] * num_of_loaders\n",
    "        for i, results in enumerate(validation_step_outputs):\n",
    "            for scores in results:\n",
    "                cnt[i] += len(scores)\n",
    "                total_metric[i] += np.sum(scores)\n",
    "            val_metric[i] = total_metric[i] / cnt[i]\n",
    "            val_metric_name = f\"val_metric_{i}th_dataset\"\n",
    "            self.log_dict({val_metric_name: val_metric[i]}, sync_dist=True)\n",
    "        self.log_dict({\"val_metric\": np.sum(total_metric) / np.sum(cnt)}, sync_dist=True)\n",
    "        print(\"Epoch:\", str(self.current_epoch), \"Step:\", str(self.global_step), \"Validation Metric:\", str(np.sum(total_metric) / np.sum(cnt)))\n",
    "        \n",
    "        if len(self.rationale_rouge_outputs) > 0:\n",
    "            avg_rouge1 = np.mean([r[\"rouge1\"].fmeasure for r in self.rationale_rouge_outputs])\n",
    "            avg_rouge2 = np.mean([r[\"rouge2\"].fmeasure for r in self.rationale_rouge_outputs])\n",
    "            avg_rougeL = np.mean([r[\"rougeL\"].fmeasure for r in self.rationale_rouge_outputs])\n",
    "        \n",
    "            self.log_dict({\n",
    "                \"val_rationale_rouge1\": avg_rouge1,\n",
    "                \"val_rationale_rouge2\": avg_rouge2,\n",
    "                \"val_rationale_rougeL\": avg_rougeL,\n",
    "            }, sync_dist=True)\n",
    "        \n",
    "            print(f\"Rationale ROUGE - R1: {avg_rouge1:.4f}, R2: {avg_rouge2:.4f}, RL: {avg_rougeL:.4f}\")\n",
    "                \n",
    "        self.rationale_rouge_outputs.clear()\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        max_iter = None\n",
    "\n",
    "        if int(self.config.get(\"max_epochs\", -1)) > 0:\n",
    "            assert len(self.config.get(\"train_batch_sizes\")) == 1, \"Set max_epochs only if the number of datasets is 1\"\n",
    "            max_iter = (self.config.get(\"max_epochs\") * self.config.get(\"num_training_samples_per_epoch\")) / (\n",
    "                self.config.get(\"train_batch_sizes\")[0] * torch.cuda.device_count() * self.config.get(\"num_nodes\", 1)\n",
    "            )\n",
    "\n",
    "        if int(self.config.get(\"max_steps\", -1)) > 0:\n",
    "            max_iter = min(self.config.get(\"max_steps\"), max_iter) if max_iter is not None else self.config.get(\"max_steps\")\n",
    "\n",
    "        assert max_iter is not None\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.get(\"lr\"))\n",
    "        scheduler = {\n",
    "            \"scheduler\": self.cosine_scheduler(optimizer, max_iter, self.config.get(\"warmup_steps\")),\n",
    "            \"name\": \"learning_rate\",\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine_scheduler(optimizer, training_steps, warmup_steps):\n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < warmup_steps:\n",
    "                return current_step / max(1, warmup_steps)\n",
    "            progress = current_step - warmup_steps\n",
    "            progress /= max(1, training_steps - warmup_steps)\n",
    "            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "        return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.args.batch_size, shuffle=True, num_workers=self.args.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.args.valid_batch_size, shuffle=False, num_workers=self.args.num_workers)\n",
    "\n",
    "    @rank_zero_only\n",
    "    def on_save_checkpoint(self, checkpoint):\n",
    "        save_path = os.path.join(self.config['result_path'], 'chartqa-checkpoint-epoch='+str(self.current_epoch)+'-'+str(self.global_step))\n",
    "        self.model.save_pretrained(save_path)\n",
    "        self.processor.save_pretrained(save_path)\n",
    "        api.upload_folder(\n",
    "            folder_path=save_path,\n",
    "            repo_id=\"YuukiAsuna/chartvqar-all\",\n",
    "            repo_type=\"model\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N57Dl2KbijtA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@title Finetune Setup\n",
    "\n",
    "from transformers import VisionEncoderDecoderConfig\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel, BartConfig\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import List\n",
    "from datasets import load_dataset, Dataset, Features, Image as HFDatasetImage, Value\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "#from pytorch_lightning.loggers import WandbLogger\n",
    "#from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Instantiate the parser\n",
    "class Config:\n",
    "    data_path = \"ahmed-masry/chartqa\"  # Path to the data file\n",
    "    train_images = \"https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/train/png/\"  # Path to the training images\n",
    "    test_images = \"https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/test/png/\"  # Path to the training images\n",
    "    valid_images = \"https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/\"  # Path to the validation images\n",
    "    output_dir = \"/kaggle/working/ChartQA_Rationale/MultiSetup\"  # Path to save checkpoints\n",
    "    max_steps = 200000  # Max number of iterations 15163\n",
    "    batch_size = 1  # Training batch size\n",
    "    valid_batch_size = 1 # Validation batch size\n",
    "    max_length = 512  # Max decoder generation length\n",
    "    num_workers = 2  # Number of workers\n",
    "    lr = 5e-5  # Learning rate\n",
    "    check_val_every_n_epoch = 1  # Run validation every n epochs\n",
    "    log_every_n_steps = 50  # Log every n steps\n",
    "    warmup_steps = 50  # Warmup steps\n",
    "    checkpoint_steps = 50000  # Save checkpoint every n steps 7581\n",
    "    gradient_clip_val = 1.0  # Gradient clipping value\n",
    "    accumulate_grad_batches = 1  # Gradient accumulation steps\n",
    "    gpus_num = 1  # Number of GPUs (use `0` for CPU)\n",
    "    nodes_num = 1  # Number of nodes\n",
    "    checkpoint_path = \"YuukiAsuna/chartvqar-all\" # \"YuukiAsuna/chartvqar\" # \"ahmed-masry/unichart-chartqa-960\"  # /content/drive/MyDrive/ChartQA_Rationale/MultiSetup/chartqa-checkpoint-epoch=2-7581 # Path to the pre-trained checkpoint\n",
    "\n",
    "\n",
    "# Use the configuration values\n",
    "args = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# #@title Finetune Test Dataset\n",
    "\n",
    "# processor = DonutProcessor.from_pretrained(args.checkpoint_path)\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(args.checkpoint_path)\n",
    "# model.to(device)\n",
    "\n",
    "# added_tokens = ['<s_rationale>', 'Answer:', 'Rationale:']\n",
    "# tokens_to_add = []\n",
    "# for token in added_tokens:\n",
    "#     if token not in processor.tokenizer.get_vocab():\n",
    "#         tokens_to_add.append(token)\n",
    "\n",
    "# processor.tokenizer.add_tokens(tokens_to_add)\n",
    "# model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "# print(f\"Added tokens: {tokens_to_add}\")\n",
    "# print(model.config.decoder.vocab_size)\n",
    "\n",
    "# train = pd.read_csv(\"/kaggle/input/chartqar-train/ChartQAR_dataset_12500-12600.csv\")\n",
    "# val = pd.read_csv(\"/kaggle/input/chartqar-test/ChartQAR_dataset_0-100.csv\")\n",
    "\n",
    "# train[\"image\"] = train[\"imgname\"].apply(lambda x: args.train_images + x)\n",
    "# val[\"image\"] = val[\"imgname\"].apply(lambda x: args.test_images + x)\n",
    "\n",
    "# train = Dataset.from_pandas(train, features=Features({\n",
    "#     \"imgname\": Value(\"string\"),\n",
    "#     \"query\": Value(\"string\"),\n",
    "#     \"label\": Value(\"string\"),\n",
    "#     \"rationale\": Value(\"string\"),\n",
    "#     \"image\": HFDatasetImage()\n",
    "# }))\n",
    "# val = Dataset.from_pandas(val, features=Features({\n",
    "#     \"imgname\": Value(\"string\"),\n",
    "#     \"query\": Value(\"string\"),\n",
    "#     \"label\": Value(\"string\"),\n",
    "#     \"rationale\": Value(\"string\"),\n",
    "#     \"image\": HFDatasetImage()\n",
    "# }))\n",
    "\n",
    "\n",
    "# train_dataset = ChartQADataset(train, images_folder = args.train_images, processor = processor, max_length=args.max_length,\n",
    "#                             split=\"train\", prompt_end_token=\"<s_answer>\", task_prefix = \"<chartqa>\"\n",
    "#                             )\n",
    "\n",
    "# val_dataset = ChartQADataset(val, images_folder = args.valid_images, processor = processor, max_length=args.max_length,\n",
    "#                             split=\"valid\", prompt_end_token=\"<s_answer>\", task_prefix = \"<chartqa>\"\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "x3OKjgl9QGmv",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@title Finetune Dataset\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(args.checkpoint_path)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(args.checkpoint_path)\n",
    "model.to(device)\n",
    "\n",
    "added_tokens = ['<s_rationale>', 'Answer:', 'Rationale:']\n",
    "tokens_to_add = []\n",
    "for token in added_tokens:\n",
    "    if token not in processor.tokenizer.get_vocab():\n",
    "        tokens_to_add.append(token)\n",
    "\n",
    "processor.tokenizer.add_tokens(tokens_to_add)\n",
    "model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "print(f\"Added tokens: {tokens_to_add}\")\n",
    "print(model.config.decoder.vocab_size)\n",
    "\n",
    "test_files = list(Path('/kaggle/input/chartqardataset/test').rglob('*.csv'))\n",
    "test = pd.concat([pd.read_csv(file) for file in test_files], ignore_index=True)\n",
    "test[\"image\"] = test[\"imgname\"].apply(lambda x: args.test_images + x)\n",
    "\n",
    "# train_files = list(Path(\"/kaggle/input/chartqardataset/train\").rglob('*.csv'))\n",
    "# train = pd.concat([pd.read_csv(file) for file in train_files], ignore_index=True)\n",
    "# train[\"image\"] = train[\"imgname\"].apply(lambda x: args.train_images + x)\n",
    "\n",
    "# train = pd.concat([train, test], ignore_index=True)\n",
    "# train = Dataset.from_pandas(train, features=Features({\n",
    "#     \"imgname\": Value(\"string\"),\n",
    "#     \"query\": Value(\"string\"),\n",
    "#     \"label\": Value(\"string\"),\n",
    "#     \"rationale\": Value(\"string\"),\n",
    "#     \"image\": HFDatasetImage()\n",
    "# }))\n",
    "\n",
    "train = Dataset.from_pandas(test, features=Features({\n",
    "    \"imgname\": Value(\"string\"),\n",
    "    \"query\": Value(\"string\"),\n",
    "    \"label\": Value(\"string\"),\n",
    "    \"rationale\": Value(\"string\"),\n",
    "    \"image\": HFDatasetImage()\n",
    "}))\n",
    "\n",
    "val = pd.read_csv(\"/kaggle/input/chartqardataset/val/ChartQAR_dataset_val_0-1056.csv\")\n",
    "val[\"image\"] = val[\"imgname\"].apply(lambda x: args.valid_images + x)\n",
    "val = Dataset.from_pandas(val, features=Features({\n",
    "    \"imgname\": Value(\"string\"),\n",
    "    \"query\": Value(\"string\"),\n",
    "    \"label\": Value(\"string\"),\n",
    "    \"rationale\": Value(\"string\"),\n",
    "    \"image\": HFDatasetImage()\n",
    "}))\n",
    "\n",
    "train_test = train.train_test_split(train_size=50_000)\n",
    "train_100k = train_test[\"train\"] \n",
    "\n",
    "train_dataset = ChartQADataset(train_100k, images_folder = args.train_images, processor = processor, max_length=args.max_length,\n",
    "                            split=\"train\", prompt_end_token=\"<s_answer>\", task_prefix = \"<chartqa>\"\n",
    "                            )\n",
    "\n",
    "val_dataset = ChartQADataset(val, images_folder = args.valid_images, processor = processor, max_length=args.max_length,\n",
    "                            split=\"valid\", prompt_end_token=\"<s_answer>\", task_prefix = \"<chartqa>\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "tQIxQhTEQMRS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#@title Finetune\n",
    "\n",
    "config = {\"max_steps\":args.max_steps,\n",
    "          \"check_val_every_n_epoch\":args.check_val_every_n_epoch,\n",
    "          \"log_every_n_steps\":args.log_every_n_steps,\n",
    "          \"gradient_clip_val\":args.gradient_clip_val,\n",
    "          \"num_training_samples_per_epoch\": len(train),\n",
    "          \"lr\":args.lr,\n",
    "          \"train_batch_sizes\": [args.batch_size],\n",
    "          \"val_batch_sizes\": [args.valid_batch_size],\n",
    "          \"num_nodes\": args.nodes_num,\n",
    "          \"warmup_steps\": args.warmup_steps,\n",
    "          \"result_path\": args.output_dir,\n",
    "          \"verbose\": True,\n",
    "        }\n",
    "\n",
    "model_module = ChartQAModule(config, processor, model, args, train_dataset, val_dataset)\n",
    "\n",
    "# wandb_logger = WandbLogger(project=\"UniChart-ChartQA\")\n",
    "# lr_callback = LearningRateMonitor(logging_interval=\"step\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=args.output_dir,\n",
    "    every_n_train_steps = args.checkpoint_steps,\n",
    "    save_last = True,\n",
    "    save_top_k = 3,\n",
    "    # monitor=\"train_loss\",\n",
    "    # mode=\"min\",\n",
    "    monitor=\"val_metric\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "      accelerator=\"gpu\",\n",
    "      devices=args.gpus_num,\n",
    "      max_steps=args.max_steps,\n",
    "      max_epochs=1, \n",
    "      check_val_every_n_epoch=args.check_val_every_n_epoch,\n",
    "      # val_check_interval=100,\n",
    "      log_every_n_steps=args.log_every_n_steps,\n",
    "      gradient_clip_val=args.gradient_clip_val,\n",
    "\n",
    "      num_nodes=args.nodes_num,\n",
    "      precision=16, # we'll use mixed precision\n",
    "      num_sanity_val_steps=0,\n",
    "      #enable_checkpointing=True,\n",
    "      default_root_dir=args.output_dir,\n",
    "      # logger=wandb_logger,\n",
    "      callbacks=[checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0Xk8m5L3owY",
    "outputId": "724f0776-ec4a-403a-9936-7189158c5ada",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# for batch_idx, batch in tqdm(enumerate(model_module.val_dataloader())):\n",
    "#     model_module.validation_step(batch, batch_idx=batch_idx, dataset_idx=0)\n",
    "#     break\n",
    "\n",
    "# model_module.on_validation_epoch_end()\n",
    "# model_module.training_step(batch, batch_idx=0)\n",
    "\n",
    "# batch = next(iter(model_module.train_dataloader()))\n",
    "\n",
    "# model_module.training_step(batch, batch_idx=0)\n",
    "# print(processor.tokenizer.batch_decode(batch[1]['rat_input_ids'][0]))\n",
    "# print(processor.tokenizer.batch_decode([i for i in batch[2][0] if i != -100]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3bfe1692f3eb43bb8a91a8c34547941c",
      "eff8e5a418d24ddd9cf41c4a4a78cf44",
      "5e6566644fc844c9951cc851ea4b37c1",
      "09e7f751a55f46baa7e6f1ea809acba1",
      "416cda7a205441a98dd87362e055470c",
      "c3167c27192f4828836b1ff947590c86",
      "5a9ca0346f744a4ebd9082a22f426270",
      "2a477f88b4b74e38a1384e66d2f9ba04",
      "a7990d6ca07240d496a57df762e2d2a2",
      "ff41de1cfbca492a8dbc9119d0799dec",
      "b9f7cf1b83fc4342b7a4283c3b7c18d5",
      "3dd8fd37ba6546ac8c343fd684f3a4e9",
      "46f255764a9049a3b49cc0ac1af91e1a",
      "b2c5f6f4ec5a4999a77c448c70984f61",
      "0ad25a4951114d8a86d798352934131c",
      "ba433ff02bac4db5b3b03b891f431115",
      "02604f8ed280460f827a3563d25359a1",
      "ee4bf0ef0ae64c459168fee844a913c4",
      "a5b1fa0b58f24fb2a3176e560111a725",
      "5b64524b4f2f4d8b9d324615fbb7f489",
      "4a3e9d091a3448b0a23d8d61e1f57daa",
      "e090619562ea4449abef0e003d1bd8b3",
      "2747da762d9b475694e5667e6eae2ff1",
      "2b5904f3b9544edca8ffe6afcf722c8a",
      "90bdb919f38946a5ba1744524839d544",
      "64d5879378c7498aaf37c3a54063a2e0",
      "6d3cf2ee88bb43c6a10673713d867188",
      "d45a8c7abe8c4507a80328ed085fe08e",
      "f05816edc2c04df484ca3a303691aa07",
      "4c8b35321c9049018a5e5b6493a4704a",
      "7296750d27e542959a6160edb3a6c629",
      "671af16a63ad4d72976ff44bc18acfd9",
      "fde430ac0f444f62a711a88779426525"
     ]
    },
    "id": "WEtqByqgmmEX",
    "outputId": "98127b5e-8d31-4c37-fd3a-842cf08c7345",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(model_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4au9NfZtQSF"
   },
   "source": [
    "# ChartQA metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"YuukiAsuna/chartvqar-all\")\n",
    "processor = DonutProcessor.from_pretrained(\"YuukiAsuna/chartvqar-all\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"HuggingFaceM4/ChartQA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "count_empty_ans = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def compute_metric(gt, pred):\n",
    "  if \"%\" in pred:\n",
    "      pred = pred.replace('%', '')\n",
    "  try:\n",
    "    gt = float(gt)\n",
    "    pred = float(pred)\n",
    "    return abs(gt - pred) / abs(gt) <= 0.05\n",
    "  except:\n",
    "    return str(gt).strip().lower() == str(pred).strip().lower()\n",
    "      \n",
    "for i in tqdm(range(len(ds['test']))):\n",
    "\n",
    "    decoder_input_ids = processor.tokenizer(f\"<chartqa> {ds['test'][i]['query']} <s_answer>\", add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "    pixel_values = processor(ds['test'][i]['image'].convert(\"RGB\"), return_tensors=\"pt\").pixel_values\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        pixel_values.to(device),\n",
    "        decoder_input_ids=decoder_input_ids.to(device),\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        num_beams=4,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "    sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "\n",
    "    if sequence.split(\"<s_answer>\")[1].strip() == \"\":\n",
    "        count_empty_ans += 1\n",
    "    \n",
    "    if compute_metric(ds['test'][i]['label'][0], sequence.split(\"<s_answer>\")[1].strip()):\n",
    "        scores.append(1)\n",
    "    else:\n",
    "        scores.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"acc: \", sum(scores)/len(scores))\n",
    "print(\"empty ans: \", count_empty_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check wrong preds\n",
    "for i, val in enumerate(scores):\n",
    "    if val == 0:\n",
    "        decoder_input_ids = processor.tokenizer(f\"<chartqa> {ds['test'][i]['query']} <s_answer>\", add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "        pixel_values = processor(ds['test'][i]['image'].convert(\"RGB\"), return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            pixel_values.to(device),\n",
    "            decoder_input_ids=decoder_input_ids.to(device),\n",
    "            max_length=model.decoder.config.max_position_embeddings,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            num_beams=4,\n",
    "            bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "        sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "        sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")       \n",
    "        print(\"Pred: \", sequence.split(\"<s_answer>\")[1])\n",
    "        print(\"Gold: \", ds['test'][i]['label'][0])\n",
    "        print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdRDyM41ZKSP"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXXXm3jKZL8k",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"YuukiAsuna/chartvqar\")\n",
    "# processor = DonutProcessor.from_pretrained(\"YuukiAsuna/chartvqar\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# i = 10\n",
    "# print(\"Query: \", val[i]['query'])\n",
    "# print(\"Label: \", val[i]['label'])\n",
    "# print(\"Rationale: \", val[i]['rationale'])\n",
    "\n",
    "# image = val[i]['image'].convert(\"RGB\")\n",
    "# decoder_input_ids = processor.tokenizer(f\"<chartqa> {val[i]['query']} <s_rationale>\", add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "# pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "# outputs = model.generate(\n",
    "#     pixel_values.to(device),\n",
    "#     decoder_input_ids=decoder_input_ids.to(device),\n",
    "#     max_length=model.decoder.config.max_position_embeddings,\n",
    "#     early_stopping=True,\n",
    "#     pad_token_id=processor.tokenizer.pad_token_id,\n",
    "#     eos_token_id=processor.tokenizer.eos_token_id,\n",
    "#     use_cache=True,\n",
    "#     num_beams=4,\n",
    "#     bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "#     return_dict_in_generate=True,\n",
    "# )\n",
    "# sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "# sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "\n",
    "# print(\"Pred rationale: \", sequence)\n",
    "\n",
    "\n",
    "# decoder_input_ids = processor.tokenizer(f\"<chartqa> {val[i]['query']} <s_answer>\", add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "# pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "# outputs = model.generate(\n",
    "#     pixel_values.to(device),\n",
    "#     decoder_input_ids=decoder_input_ids.to(device),\n",
    "#     max_length=model.decoder.config.max_position_embeddings,\n",
    "#     early_stopping=True,\n",
    "#     pad_token_id=processor.tokenizer.pad_token_id,\n",
    "#     eos_token_id=processor.tokenizer.eos_token_id,\n",
    "#     use_cache=True,\n",
    "#     num_beams=4,\n",
    "#     bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "#     return_dict_in_generate=True,\n",
    "# )\n",
    "# sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "# sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "# print(\"Pred answer: \", sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "FEdVdMnEqlb2",
    "outputId": "8e61f607-3756-4a27-db8c-671c79fcd886",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# val[10]['image']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D0c0ihw1t6P"
   },
   "source": [
    "# Push to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ls ChartQA_Rationale/MultiSetup/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7627356,
     "sourceId": 12114293,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7627361,
     "sourceId": 12114299,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7642818,
     "sourceId": 12136216,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02604f8ed280460f827a3563d25359a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09e7f751a55f46baa7e6f1ea809acba1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff41de1cfbca492a8dbc9119d0799dec",
      "placeholder": "​",
      "style": "IPY_MODEL_b9f7cf1b83fc4342b7a4283c3b7c18d5",
      "value": " 160/3704 [01:44&lt;38:35,  1.53it/s, v_num=2]"
     }
    },
    "0ad25a4951114d8a86d798352934131c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a3e9d091a3448b0a23d8d61e1f57daa",
      "placeholder": "​",
      "style": "IPY_MODEL_e090619562ea4449abef0e003d1bd8b3",
      "value": " 3744/3744 [52:31&lt;00:00,  1.19it/s]"
     }
    },
    "2747da762d9b475694e5667e6eae2ff1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2b5904f3b9544edca8ffe6afcf722c8a",
       "IPY_MODEL_90bdb919f38946a5ba1744524839d544",
       "IPY_MODEL_64d5879378c7498aaf37c3a54063a2e0"
      ],
      "layout": "IPY_MODEL_6d3cf2ee88bb43c6a10673713d867188"
     }
    },
    "2a477f88b4b74e38a1384e66d2f9ba04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b5904f3b9544edca8ffe6afcf722c8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d45a8c7abe8c4507a80328ed085fe08e",
      "placeholder": "​",
      "style": "IPY_MODEL_f05816edc2c04df484ca3a303691aa07",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "3bfe1692f3eb43bb8a91a8c34547941c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eff8e5a418d24ddd9cf41c4a4a78cf44",
       "IPY_MODEL_5e6566644fc844c9951cc851ea4b37c1",
       "IPY_MODEL_09e7f751a55f46baa7e6f1ea809acba1"
      ],
      "layout": "IPY_MODEL_416cda7a205441a98dd87362e055470c"
     }
    },
    "3dd8fd37ba6546ac8c343fd684f3a4e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_46f255764a9049a3b49cc0ac1af91e1a",
       "IPY_MODEL_b2c5f6f4ec5a4999a77c448c70984f61",
       "IPY_MODEL_0ad25a4951114d8a86d798352934131c"
      ],
      "layout": "IPY_MODEL_ba433ff02bac4db5b3b03b891f431115"
     }
    },
    "416cda7a205441a98dd87362e055470c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "46f255764a9049a3b49cc0ac1af91e1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02604f8ed280460f827a3563d25359a1",
      "placeholder": "​",
      "style": "IPY_MODEL_ee4bf0ef0ae64c459168fee844a913c4",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "4a3e9d091a3448b0a23d8d61e1f57daa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c8b35321c9049018a5e5b6493a4704a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a9ca0346f744a4ebd9082a22f426270": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b64524b4f2f4d8b9d324615fbb7f489": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5e6566644fc844c9951cc851ea4b37c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a477f88b4b74e38a1384e66d2f9ba04",
      "max": 3704,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a7990d6ca07240d496a57df762e2d2a2",
      "value": 160
     }
    },
    "64d5879378c7498aaf37c3a54063a2e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_671af16a63ad4d72976ff44bc18acfd9",
      "placeholder": "​",
      "style": "IPY_MODEL_fde430ac0f444f62a711a88779426525",
      "value": " 3744/3744 [40:15&lt;00:00,  1.55it/s]"
     }
    },
    "671af16a63ad4d72976ff44bc18acfd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d3cf2ee88bb43c6a10673713d867188": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "7296750d27e542959a6160edb3a6c629": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "90bdb919f38946a5ba1744524839d544": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c8b35321c9049018a5e5b6493a4704a",
      "max": 3744,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7296750d27e542959a6160edb3a6c629",
      "value": 3744
     }
    },
    "a5b1fa0b58f24fb2a3176e560111a725": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7990d6ca07240d496a57df762e2d2a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b2c5f6f4ec5a4999a77c448c70984f61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5b1fa0b58f24fb2a3176e560111a725",
      "max": 3744,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5b64524b4f2f4d8b9d324615fbb7f489",
      "value": 3744
     }
    },
    "b9f7cf1b83fc4342b7a4283c3b7c18d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba433ff02bac4db5b3b03b891f431115": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "c3167c27192f4828836b1ff947590c86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d45a8c7abe8c4507a80328ed085fe08e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e090619562ea4449abef0e003d1bd8b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ee4bf0ef0ae64c459168fee844a913c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eff8e5a418d24ddd9cf41c4a4a78cf44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3167c27192f4828836b1ff947590c86",
      "placeholder": "​",
      "style": "IPY_MODEL_5a9ca0346f744a4ebd9082a22f426270",
      "value": "Epoch 2:   4%"
     }
    },
    "f05816edc2c04df484ca3a303691aa07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fde430ac0f444f62a711a88779426525": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff41de1cfbca492a8dbc9119d0799dec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
